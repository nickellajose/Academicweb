---
title: Haunted Places
author: ''
date: '2023-10-11'
slug: haunted-places
categories:
  - R
tags:
  - R
subtitle: ''
summary: ''
authors: [Nickella Jose]
lastmod: '2023-10-11T14:28:11+02:00'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

Since we are in the month of Halloween, lets take a look at some of the haunted places in the US and their characteristics.
```{r, include=FALSE}
library("tidytuesdayR")
library(leaflet)
library(plotly)
library(tidyr)
library(dplyr)
library(tidyverse)
library(usmap)
library(ggplot2)
library(sf)
library(broom)
library(sp)
library(tm)
library(topicmodels)
library(RColorBrewer)
library(grDevices)
library(ggthemes)
library(colorspace)
library(stringr)
library(scales)
library(viridis)

#loading data
haunted_places<- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-10/haunted_places.csv')
#rm NA
haunted_places<-na.omit(haunted_places)
#agg by state and numbe rof haunted places
by_state<-haunted_places %>% 
  group_by(state) %>% 
  summarise(stat_count=n())
#shp us map
mg_dat<-merge(haunted_places,by_state)
usa <-st_read("/Users/cb_2022_us_state_500k")

usa2<- usa[c(5,6,10)]

z<- mg_dat %>% group_by(state) %>% 
   summarise(avg=mean(stat_count))

usa_map<-merge(usa2,z,by.x="NAME", by.y= "state")

usa_map<-usa_map %>% mutate(cat=ifelse(avg<=50,"0-50",
                                     ifelse(avg<=100,"50-100",
                                            ifelse(avg<=200,"100-200",
                                                   ifelse(avg<=300,"200-300","300-1002")))))

usa_map$cat <-factor(usa_map$cat, levels = c("0-50", "50-100", "100-200", "200-300" , "300-1002" ))



cmap<-diverging_hcl(5,palette="Blue-Red2")



```

Here the number of haunted locations are aggregated by state.The Western Great Lakes and the Northeastern regions found in eastern USA is home to some of the most haunted states.
These include Illinois (IL), Michigan (MI),New York (NY) and Ohio (OH).

```{r,echo=FALSE , warning=FALSE}




ggplot(usa_map)+
  geom_sf(aes(fill=cat,color=cat))+scale_fill_manual(values=c("#4A6FE3", "#9DA8E2" ,"#E2E2E2", "#E495A5", "#D33F6A"))+
  scale_colour_manual(values=c("gray70" ,"gray70", "gray70", "gray70", "gray70"))+
  labs(fill="Number of Haunted Locations",color="Number of Haunted Locations")+
        scale_x_continuous(limits = c(-125, -67))+
        scale_y_continuous(limits = c(22, 50))+theme_map()+theme(legend.direction = "horizontal")+
   geom_sf_text(data=usa_map,aes(label=STUSPS),colour="white", size=2)+guides(fill = guide_legend( title.position = "bottom"),colour = guide_legend( title.position = "bottom"), 
                                                                               title.theme =element_text(size = 10, face = "bold",colour = "gray70",angle = 0))+
                                                                                ggtitle(label = "Number of Haunted Locations in the USA by State")






```

This bar plot highlights the top 15 states with the most haunted locations, with 1002 haunted sites.
California has the highest number of haunted locations followed by Texas with 637 haunted sites. 

```{r,echo=FALSE}
#Top 15 haunted states
top15<- by_state %>% top_n(15,wt=stat_count) 


top15$state <-as.factor(top15$state)


top15%>% 
  ggplot()+
  geom_bar(aes(x=reorder(state,-stat_count),y=stat_count, fill=state),stat = "identity", show.legend = FALSE)+ 
  coord_flip()+labs(title = "Top 15 Most Haunted States in the USA", x="State", y="Number of Haunted Locations")+ scale_y_continuous(breaks = seq(0,1100,100), limits=c(0,1100))+
  scale_fill_manual(values=c("#8E063B","#023FA5","#023FA5", "#7D87B9" ,"#023FA9", "#7D87B9", "#7D87B9", "#D6BCC0", "#023FA5", "#D6BCC0", "#D6BCC0", "#D6BCC0","#D6BCC0","#023FA5","#023FA9"))





```

The interactive map below shows the haunted sites for the states with 200 or more haunted locations.
Each red marker (circle) represents a haunted site in a specific state, when you zoom-in you can see an approximate location in the city in which it is located.
To read the description of the haunted location  click on the marker.

```{r,echo=FALSE}
## Most haunted regions 


#subset data for states whit most haunted locations

most_haunted<- mg_dat %>% filter(stat_count>=200)
 
leaflet() %>% 
  addTiles()%>% 
    addScaleBar() %>% 
  setView(lng = -96.25, lat = 39.50, zoom = 4.4) %>% 
  addMiniMap() %>% 
  addCircleMarkers(data = most_haunted,
                   lng=most_haunted$city_longitude,
                   lat=most_haunted$city_latitude,
                   weight = 1,
                   radius = 5,
                   color="Red",
                   popup = ~description)


```



```{r, echo=FALSE}


library(data.table)
library(dplyr)
library(ggplot2)
library(slam)         # New: needed for perplexity calc
library(gutenbergr)   # New: data
library(quanteda)     # New: to process text
library(topicmodels)  # New: to fit topic models
library(word2vec)     # New: to fit word embeddings
# --------------------------------------------------------

library(quanteda) 

d<- mg_dat %>% filter(state_abbrev==c("MI","WI","MO","IL","IN","KY","OH","PA","NY","MA"))

df<-data.frame(descrip=d$description)

# =======================================
# Pre-process
# =======================================
Corpus <- corpus(x = df,
                       text_field = "descrip",
                       docid_field = 'doc_id')

# Tokenize & clean from particular types of words
mytokens <- tokens(x = Corpus, 
                   remove_punct = TRUE, 
                   remove_numbers = TRUE, 
                   remove_symbols = TRUE,
                   remove_url = TRUE,
                   padding = FALSE)

mytokens <-   tokens_remove(x = mytokens,
                            stopwords("en"), 
                            padding = FALSE)


mytokens <-   tokens_select(x = mytokens,selection = 'remove',
                            valuetype = 'glob', 
                            pattern = '@', 
                            padding = FALSE)

# Make tokens lowercase
mytokens <- tokens_tolower(x = mytokens)

# Create document term matrix
dtm <- dfm(x = mytokens)

# Exclude words with too low frequency
dtm <- dfm_trim(dtm, min_termfreq = 5)

# Exclude documents with too low frequency
rowsums <- rowSums(dtm)
keep_ids <- which(rowsums>=10)
dtm <- dtm[keep_ids,]

# Report final dimensionality of data set
dim(dtm)


# --------------------------------------------------------
# Topic modeling in R
# --------------------------------------------------------

# Fit LDA
# - 30 topics
# - Estimation algorithm: Gibbs
# - 1000 iterations
# - Set seed for replicability
# - Print iter every 100th
K <- 30 #50
mylda <- LDA(x = dtm, 
             k = K, 
             method="Gibbs",
             control=list(iter = 1000, 
                          seed = 1, 
                          verbose = 100))

# Inspect top 10 from each topics
get_terms(mylda, 10)
get_terms(mylda, 10)[,1:10] # Only first 10 topics

# To get a more granular view, extract the probabilities 
mylda_posterior <- topicmodels::posterior(object = mylda)
topic_distr_over_words <- mylda_posterior$terms

topic_distr_over_words_dt <- data.table(topic=1:K, 
                                        topic_distr_over_words) #datatable
topic_distr_over_words_dt <- melt.data.table(topic_distr_over_words_dt, 
                                             id.vars = 'topic') #tidyR

# Tidyr/Dplyr way of extracting top 10 rows by group
top10per_topic <- topic_distr_over_words_dt %>% 
  group_by(topic) %>% 
  slice_max(order_by = value, n = 10)

# data.table way of extracting top 10 rows by group
topic_distr_over_words_dt <- topic_distr_over_words_dt[order(value,decreasing = T)]
top10per_topic <- topic_distr_over_words_dt[,head(.SD,10),by='topic']

# Plot probability over words for a few topics.
ggplot(top10per_topic[topic %in% c(6,7,10)],aes(y=factor(variable),x=value)) + 
  geom_bar(stat = 'identity', position = 'dodge') + 
  facet_wrap(~topic,scales = 'free')

# To validate labeling; identify documents with highest proportion on any given topic
doc_topic_proportions <- mylda_posterior$topics
dim(doc_topic_proportions)
head(doc_topic_proportions)
doc_topic_proportions_dt <- data.table(doc_id=mylda@documents,
                                       doc_topic_proportions)
colnames(doc_topic_proportions_dt)[2:ncol(doc_topic_proportions_dt)] <- paste0('Topic',colnames(doc_topic_proportions_dt)[2:ncol(doc_topic_proportions_dt)])
#doc_topic_proportions_dt[,row_id := 1:.N]
doc_topic_proportions_dt[order(Topic7,decreasing = T)][1:5,]

# Select rows from original data...
bernie_trump[doc_id==3546]
bernie_trump[doc_id==4401]


```


Data From #TidyTuesday, Find@ https://github.com/rfordatascience/tidytuesday/tree/master/data 